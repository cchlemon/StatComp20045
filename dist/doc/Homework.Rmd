---
title: "Homework"
author: '20045'
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Question
1,show data by tables

2,show data by figures

3,outpot mathematical formulas

## Answer

1,Recently,researchers have carried out an investigation about three species of iris and some kinds of cars.We selected twenty data about iris and some data about cars as follows.

# cars
```{r results='asis',echo=FALSE}
knitr::kable(tail(mtcars))
```
# iris
```{r results='asis',echo=FALSE}
knitr::kable(iris[10:30,])
```


2,There are some data about weight and group

```{r echo=FALSE}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14) 
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69) 
group <- gl(2, 10, 20, labels = c("Ctl","Trt")) 
weight <- c(ctl, trt) 
lm.D9 <- lm(weight ~ group)
par(mfrow=c(2,2))
win.graph(width=4.875, height=2.5,pointsize=8)
plot(lm.D9)
```


3,limitation,integral and differential

$$
\lim _{n \rightarrow \infty} \frac{1}{n^{2}+n+1}+\frac{2}{n^{2}+n+2}+\cdots+\frac{n}{n^{2}+n+n}
$$

$$
\int\left(\sum_{k} a_{k} 1_{S_{k}}\right) d \mu=\sum_{k} a_{k} \int 1_{S_{k}} d \mu=\sum_{k} a_{k} \mu\left(S_{k}\right)
$$

$$
f\left(x, \frac{d^{n} y}{d x^{n}}, \frac{d^{(n-1)} y}{d x^{(n-1)}}, \cdots, \frac{d y}{d x}, y\right)=0
$$


## Question

Exercises 3.3, 3.9, 3.10, and 3.13 (pages 94-95, Statistical
Computating with R).

## Answer

3.3
when a,b =2,$F(x)=1-\left(\frac{2}{x}\right)^{2}, \quad x \geqslant 2$.then:
$$
x=\frac{2}{(1-F(x))^{\frac{1}{2}}}
$$
then:
$$
F^{-1}(x)=\frac{2}{(1-x)^{\frac{1}{2}}}
$$
make $u \sim u(0,1)$
$$
x=\frac{2}{(1-u)^{\frac{1}{2}}}
$$
then
$$
x=\frac{2}{u^{\frac{1}{2}}}
$$


then
$$
\frac{d(F(x))}{d x}=f(x)=\frac{d\left(1-\left(\frac{2}{x}\right)^{2}\right)}{d x}=8 \frac{1}{x^{3}}
$$

codes as follows:
```{r}
n<-1e4
k<-0;j<-0
y<-numeric(n)
while(k<n)
{x<-2/runif(1)^0.5;j<-j+1
if(x<24){k<-k+1;y[k]<-x}
}
hist(y,breaks=seq(0,24,2),prob=TRUE,main=expression(f(x)==8/x^3))
lines(z<-seq(0,24,0.1),8/z^3)
```


3.9
codes as follows
```{r}
k<-0
n<-1e4
e<-numeric(n)
while(k<n){
  u1<-runif(1,-1,1)
  u2<-runif(1,-1,1)
  u3<-runif(1,-1,1)
  if(abs(u3)>=abs(u2)
     &
     abs(u3)>=abs(u1)){
    k<-k+1
    e[k]<-u2
  }
  else{
    k<-k+1
    e[k]<-u3
  }
}
hist(e,prob=TRUE,main=expression(f(x)==3/4*(1-x^2)))
c<-seq(-1,1,0.1)
lines(c,3/4*(1-c^2))
```

3.10

make$X$denotes the random variable.make$Z=|X|$,so$U_1,U_2,U_3\stackrel{iid}\sim u(-1,1)$,so $|U_1|,|U_2|,|U_3|\stackrel{iid}\sim u(-1,1)$,and the density functionis $f_{|U_1|,|U_2|,|U_3|}(x,y,z)=1$.noting $B=\{|U_3|\geqslant|U_2|,|U_3|\geqslant|U_1|\}$,then
$$
p(Z\leqslant t)=p(Z\leqslant t,B)+p(Z\leqslant t,\stackrel{-}B)=p(|U_2|\leqslant t,B)+p(|U_3|\leqslant t)-p(|U_3|\leqslant t,B)
$$
and
$$
p(|U_2|\leqslant t,B)=\int_0^td|u_2|\int_{|U_2|}^1d|u_3|\int_0^{|u_3|}d|u_1|
$$
$$
=\int_0^td|u_2|\int_{|U_2|}^1|u_3|d|u_3|
$$
$$
=\int_0^t \frac{1}{2}(1-|u_2|^2)d|u_2|
$$
$$
=\frac{1}{2}\int_0^t(1-|u_2|^2)d|u_2|
$$
$$=\frac{1}{2}t-\frac{1}{6}t^3$$
$$p(|U_3|\leqslant t)=t$$
$$p(|U_3|\leqslant t,B)=\int_0^td|u_3|\int_0^{|u_3|}d|u_2|\int_0^{|u_3|}d|u_1|$$
$$=\int_0^t|u_3|^2d|u_3|$$
$$=\frac{1}{3}t^3$$
$\therefore p(Z\leqslant t)=p(|X|\leqslant t)=-\frac{1}{2}t^3+\frac{3}{2}t$

$\therefore F_{|X|}(t)=-\frac{1}{2}t^3+\frac{3}{2}t$

$\therefore f_{|X|}(t)=\frac{dF_{|X|}(t)}{dt}=\frac{3}{2}(1-t^2)$

$\because \mbox{the density function of X}$

$\therefore f_X(t)=\frac{3}{4}(1-t^2) $


3.13
when$r=4, \beta=2$,then $F(y)=1-\left(\frac{2}{2+y}\right)^{4}, y \geqslant 0$
compute the density function:
$$
y=\frac{2}{(1-F(y))^{\frac{1}{4}}}-2
$$
then:
$$
F^{-1}(x)=\frac{2}{(1-y)^{\frac{1}{4}}-2}
$$
so
$$
x=\frac{2}{u^{\frac{1}{4}}}-2
$$

codes as follows
```{r}
k<-0
n<-1e4
y<-numeric(n)
while(k<n){
  x<-2/runif(1)^0.25-2
  if(x<10){k<-k+1;y[k]<-x}
}
hist(y,prob=TRUE)
d<-seq(0,10,0.1)
lines(d,64/(2+d)^5)
```


## Question

Exercises 5.1, 5.7, and 5.11 (pages 149-151, Statistical
Computating with R).

## Answer

5.1 
First step: compute the integral
$$ 
\begin{align}
\int_0^{\frac{\pi}{3}}sint\mathrm{d}t\\
&= -cost|_0^{\frac{\pi}{3}}\hfill\\
&= -cos(\frac{\pi}{3})-(-cos(0))\\
&= -\frac{1}{2}-(-1)\\
&= \frac{1}{2}\\
\end{align}
$$
Second step:estimate
$$
\begin{align}
\int_0^{\frac{\pi}{3}}sint\mathrm{d}t\\
&= \int_0^{\frac{\pi}{3}} \frac{\pi}{3}sint \frac{1}{\frac{\pi}{3}}\mathrm{d}t\\
&= \int_0^{\frac{\pi}{3}} \frac{\pi}{3}sint f_U (t)\mathrm{d}t\\
\end{align}
$$
and$f_U (t)=\frac{1}{\frac{\pi}{3}}$is the uniformly distribution u(0,$\frac{\pi}{3}$) function,U is the random variable

$\therefore$

$$
\begin{align}
\int_0^{\frac{\pi}{3}}sint\mathrm{d}t
&= E\left(g(U)\right)
\end{align}
$$
and $g(U)=\frac{\pi}{3} sin\left(U\right)$

then generate random number of$g(U)$.codes as follows

```{r}
set.seed(1)
u<-runif(1e4,0,pi/3)
y<-pi/3*sin(u)
mean(y)
```
$\therefore\mbox{estimteo is }0.4997336,\mbox{true value is }0.5,\mbox{among the erros}$



5.7 the simple Monte Carlo method:
$$
\begin{align}
\theta&=\int_0^1e^x\mathrm{d}x\\
&=\int_0^1 1\times e^x \mathrm{d}x\\
&=\int_0^1 f(x)\times e^x \mathrm{d}x\\
&=E\left(e^U\right)
\end{align}
$$


antithetic variate approach:$\hat{\theta_2}=\frac{1}{n}\sum_{i=1}^{n} \left(e^{1-X_i}+e^{X_i}\right)$

$\mbox{codes as follows}$
```{r}
MC_modify_variance<- function(x, R = 10000, antithetic = TRUE) {
  u <- runif(R/2,0,x)
  if (!antithetic) v <- runif(R/2,0,x) else v <- 1 - u
  u <- c(u, v)
  cdf <- numeric(length(x))
  for (i in 1:length(x)) {
    g <- x[i] * exp(u * x[i])
    cdf[i] <- mean(g)
  }
  cdf
}
m <- 1000
theta_1 <- theta_2 <- numeric(m)
x <- 1
for (i in 1:m) {
  theta_1[i] <- MC_modify_variance(x, R = 1000, anti = FALSE)
  theta_2[i] <- MC_modify_variance(x, R = 1000)
}
mean(theta_2)
mean(theta_1)
c(sd(theta_1),sd(theta_2),sd(theta_2)/sd(theta_1))
```


5.11 equation (5.11)
$$
var(\hat{\theta_c})=var(\hat{\theta_2})+c^2 var(\hat{\theta_1}-\hat{\theta_2})+2ccov(\hat{\theta_2},\hat{\theta_1}-\hat{\theta_2})
$$
$\mbox{iif}\\$
$$
\begin{align}
c&=-\frac{2cov(\hat{\theta_2},\hat{\theta_1}-\hat{\theta_2})}{2 var(\hat{\theta_1}-\hat{\theta_2})}\\
&=\frac{cov(\hat{\theta_2},\hat{\theta_2}-\hat{\theta_1})}{var(\hat{\theta_1}-\hat{\theta_2})}\\
&=\frac{var(\hat{\theta_2})-cov(\hat{\theta_2}-\hat{\theta_1})}{var(\hat{\theta_2})+var(\hat{\theta_1})-2cov(\hat{\theta_2},\hat{\theta_1})}\\
\end{align}
$$
$var(\hat{\theta_c})\mbox{ got the minimum}$


## Question 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on$(1,+\infty)$ and are 'close' to
$$
g(x)=\frac{x^2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}},x>1
$$
Which of your two importance functions should produce the smaller variance
in estimating
$$
\int_1^{+\infty}\frac{x^2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx
$$
by importance sampling? Explain.

## Answer 5.13

1,find$f_1$


According to the structure of g(x),thus we choose the$f_1(x)=kxe^{-\frac{x^2}{2}}$;then we caculate the value of k
$$
\begin{align}
\int_1^{+\infty}kx e^{-\frac{x^2}{2}}dx&=k\int_1^{+\infty}x e^{-\frac{x^2}{2}}dx\\
&=\frac{1}{2}k \int_1^{+\infty}e^{-\frac{x^2}{2}}dx^2\\
&=\frac{1}{2}k \int_1^{+\infty}e^{-\frac{u}{2}}du(x^2=u)\\
&=\frac{1}{2}k \frac{e^{-\frac{u}{2}}}{-\frac{1}{2}}|_1^{{+\infty}}\\
&=k e^{-\frac{1}{2}}
\end{align}
$$
making $k e^{-\frac{1}{2}}=1$,thus $k=e^{\frac{1}{2}}$

so$f_1(x)=e^{\frac{1}{2}} x e^{-\frac{x^2}{2}},x>1$ it is a density function.

next step:we employ inverse transformation method to generate random number of $X\sim f_1(x)$

First:caculate distribution function of X

$$
\begin{align}
F_X(u)&=\int_1^u e^{\frac{1}{2}} x e^{-\frac{x^2}{2}}dx\\
&=\frac{1}{2}e^{\frac{1}{2}}\int_1^ue^{-\frac{x^2}{2}}dx^2\\
&=\frac{1}{2}e^{\frac{1}{2}}\int_1^ue^{-\frac{t}{2}}dt(x^2=t)\\
&=\frac{1}{2}e^{\frac{1}{2}}\frac{e^{-\frac{t}{2}}}{-\frac{1}{2}}|_1^{{u}}\\
&=1-e^{\frac{1}{2}(1-u^2)}
\end{align}
$$
Second step: caculate inverse function of$F_X(u)$
$$
\begin{align}
F_X(u)=1-e^{\frac{1}{2}(1-u^2)}&\iff 1-F_X(u)=e^{\frac{1}{2}(1-u^2)}\\
&\iff 1-u^2=2ln(1-F_X(u))\\
&\iff u=\sqrt{1-2ln(1-F_X(u))}
\end{align}
$$
$\therefore F_X^{-1}(u)=\sqrt{1-2ln(1-u)},$ making$U\sim u(0,1)$,then $F_X^{-1}(U)=\sqrt{1-2ln(1-U)}=\sqrt{1-2ln(U)}$ (noting:1-U has a similar distribution function with U).

so
$$
\begin{align}
&\int_1^{+\infty}\frac{x^2}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx\\
&=\int_1^{+\infty}\frac{g(x)}{f_1(x)} f_1(x)dx\\
&=\int_1^{+\infty}\frac{1}{e^{\frac{1}{2}} \sqrt{2\pi}}x f_1(x)dx\\
&=E\left(\frac{1}{e^{\frac{1}{2}} \sqrt{2\pi}}x\right)\\
&=\frac{1}{e^{\frac{1}{2}} \sqrt{2\pi}}E(x)
\end{align}
$$
so the estimator of the integral is $\frac{1}{e^{\frac{1}{2}} \sqrt{2\pi}}\frac{1}{n}\sum_{i=1}^n X_i,X_i=\sqrt{1-2ln(U)},U\sim u(0,1)$

codes as follows
```{r}
n<-1e4
set.seed(1)
u<-runif(n)
x_1<-(1-2*log(u))^0.5
int_1<- 1/(exp(0.5)*sqrt(2*pi))*mean(x_1)
var_int_1<- 1/(exp(1)*2*pi*n)*var(x_1)
sd_int_1<- 1/(exp(0.5)*sqrt(2*pi*n))*sd(x_1)
round(print(c(int_1,var_int_1,sd_int_1)),8)
```

2,find$f_2$

According to the structure of g(x),thus we choose the$f_2(x)=\frac{1}{x^2},x>1$

$\because$
$$\int_1^{+\infty}f_2(x)dx=1$$
$\therefore$ it is a density function

then we employ inverse transformation method to generate random number of $X\sim f_2(x)$

First step:caculate distribution function

$$
F_X(u)=\int_1^u f_2(x)dx=1-\frac{1}{u}
$$
Second step:caculate inverse function of$F_2(X)$
$$
F_X(u)=1-\frac{1}{u}\iff u=\frac{1}{1-F_X(u)}
$$
$\therefore F_X^{-1}(u)=\frac{1}{1-u}$,making $U\sim u(0,1),\therefore F_X^{-1}(U)=\frac{1}{1-U}=\frac{1}{U}$(noting:1-U has a similar distribution function with U)

$\therefore$
$$
\begin{align}
\int_1^{+\infty}g(x)dx&=\int_1^{+\infty}\frac{g(x)}{f_1(x)} f_1(x)dx\\
&=E\left(\frac{x^4}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\right)
\end{align}
$$
$\therefore$ the estimator of the integral is$\frac{1}{n}\sum_{i=1}^n \frac{X_i^4}{\sqrt{2\pi}} e^{-\frac{X_i^2}{2}},X_i=\frac{1}{U},U\sim u(0,1)$

codes as follows
```{r}
n<-1e4
set.seed(1)
u<-runif(n)
x_2<- 1/u
t<- x_2^4*exp(-0.5*x_2^2)/sqrt(2*pi)
int_2<- mean(t)
var_int_2<- var(t)/n
sd_int_2<- sqrt(var_int_2)
round(print(c(int_2,var_int_2,sd_int_2)),8)
```

3,analyze$f_1(x),f_2(x)$

```{r}
a<-seq(1,5,0.01)
g<- a^2/sqrt(exp(a^2)*2*pi)
f_1<- a/sqrt(exp(a^2)*2*pi)
f_2<- 1/a^2
plot(a,g,type = "l",col=1)
lines(a,f_1,col=2)
lines(a,f_2,col=3)
```

black line represents $g(x)$;red line represents $f_1(x)$;green line represents $f_2(x)$.

According to the above picture,compared with$f_2(x)$,$f_1(x)$ is more similar to$g(x)$. Actually,we can know the fact by observing the structure of functions.So this is why the variance of $f_1(x)$ is more little than the variance of $f_2(x)$


## Question 5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

## Answer

$$
\int_0^1 \frac{1}{1+x^2} e^{-x}dx=\sum_{i=0}^4 \int_{\frac{i}{5}}^{\frac{i+1}{5}} \frac{1}{1+x^2} e^{-x}dx
$$
$for \forall i,making \int_{\frac{i}{5}}^{\frac{i+1}{5}} k_i e^{-x}dx=1$

so

$$
k_i=\frac{1}{e^{-\frac{i}{5}}-e^{-\frac{i+1}{5}}}
$$
so,for$\forall i=0,1,2,3,4;$

$$
f_{X_i}(x)=\frac{1}{e^{-\frac{i}{5}}-e^{-\frac{i+1}{5}}} e^{-x} I[\frac{i}{5}<x<\frac{i+1}{5}]
$$
is a density function.

so,
$$
\begin{align}
\int_0^1 \frac{1}{1+x^2} e^{-x}dx&=\sum_{i=0}^4 \int_{\frac{i}{5}}^{\frac{i+1}{5}} \frac{1}{1+x^2} e^{-x}dx\\
&=\sum_{i=0}^4 \int_{\frac{i}{5}}^{\frac{i+1}{5}} \frac{e^{-\frac{i}{5}}-e^{-\frac{i+1}{5}}}{1+x^2} f_{X_i}(x)dx\\
&=\sum_{i=0}^4 E\left(\frac{1}{1+X_i^2}\right)k_i^{-1}
\end{align}
$$
noting:$X_i\sim  f_{X_i}(x)$

now we employ the inverse transformation method to generate random number of $X_i$.

$$
F_{X_i}(u)=\int_{\frac{i}{5}}^u k_i e^{-x}dx=k_i(e^{-\frac{i}{5}}-e^{-u})
$$
$\therefore$
$$
F_{X_i}^{-1}(u)=-ln(e^{-\frac{i}{5}}-\frac{u}{k_i})
$$
$\therefore$
$$
X_i =-ln(e^{-\frac{i}{5}}-\frac{U}{k_i});U\sim u(0,1)
$$
making $Y_i=\frac{1}{1+X_i^2};\theta=\int_0^1 \frac{1}{1+x^2} e^{-x}dx$
$\therefore$
$$
\theta=\sum_{i=0}^4 E\left(Y_i\right)k_i^{-1}\\
\hat{\theta}=\sum_{i=0}^4 k_i^{-1} \frac{1}{n}\sum_{j=1}^n Y_{ij}\\
D(\hat{\theta})=\sum_{i=0}^4 k_i^{-2} \frac{1}{n} D(Y_{ij})
$$
coeds as follows:
```{r}
m<-5;n<-1e4
k<-numeric(m);theta_i<-numeric(m);var_i<-numeric(m)
y<-x<-matrix(numeric(5*n),ncol=5,nrow=n)
set.seed(1)
u<-runif(n)
for(i in 0:4){
  k[i+1]<-(exp(-i/5)-exp(-(i+1)/5))^(-1)
  x[,i+1]<- -log(exp(-i/5)-u/k[i+1])
  y[,i+1]<- (1+x[,i+1]^2)^(-1)
  theta_i[i+1]<- mean(y[,i+1])/k[i+1]
  var_i[i+1]<- var(y[,i+1])/(n*k[i+1]^2)
}
hat_theta<- sum(theta_i)
var_hat_theta<-sum(var_i)
sd_hat_theta<-sqrt(var_hat_theta)
print(round(c(hat_theta,var_hat_theta,sd_hat_theta),8))

```
compared with the result of Example 5.10,there was a significant reduction in variance.


## Question 6.4

Suppose that X1,...,Xn are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter µ. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.

## Answer

$\because$  X1,...,Xn are a random sample from a from a lognormal distribution with unknown parameters.$\therefore$ making $Y_i=ln\left(X_i\right)$,thus $Y_i\overset{iid}{\sim}N\left(\mu,\sigma^2\right)$.

$\because$ there are unknown parameters,$therefore$ there are two cases as follows:

First case:when there is a more smaller sample size,we employ t distribution to solve the problem.

Second case:when there is a more bigger sample size,we employ normal distribution to solve the problem.

According to several knowledge of statistics,there are some facts as follows:

$$
\frac{\bar{Y_i}-\mu}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1)
$$
$$
\frac{(n-1)s^2}{\sigma^2}\sim \chi^2 (n-1);S=\sqrt{S^2}=\sqrt{\frac{\sum_{i=1}^n(Y_i-\bar{Y})}{n-1}}
$$
so

$$
\frac{\sqrt{n}(\bar{Y_i}-\mu)}{S}\sim t(n-1)
$$
$$
\frac{\sqrt{n}(\bar{Y_i}-\mu)}{S}\sim N(0,1)(n\rightarrow +\infty)
$$
so,the confidence interval is

First case:
$$
\left(\bar{Y_i}-\frac{S* t_{\frac{\alpha}{2}}(n-1)}{\sqrt{n}},\bar{Y_i}+\frac{S* t_{\frac{\alpha}{2}}(n-1)}{\sqrt{n}}\right)
$$

Second case:
$$
\left(\bar{Y_i}-\frac{S* Z_{\frac{\alpha}{2}}}{\sqrt{n}},\bar{Y_i}+\frac{S* Z_{\frac{\alpha}{2}}}{\sqrt{n}}\right)(n\rightarrow +\infty)
$$
codes as follows:

```{r}
mu<-0;sigma<-1;m<-1e4;n1<-10;n2<-100
y_1<-numeric(m);y_2<-numeric(m)
sd_1<-numeric(m);sd_2<-numeric(m)
t_0.025_9<-2.262;z_0.025<-1.96
# first case:t distribution
for(i in 1:m){
  a<-rt(n1,n1-1)
  y_1[i]<-mean(a)
  sd_1[i]<-sd(a)
}
t_theta_1<- y_1-sd_1*t_0.025_9/sqrt(n1)
t_theta_2<- y_1+sd_1*t_0.025_9/sqrt(n1)
d1<-0
for(i in 1:m){
  if(t_theta_1[i]<=mu & mu<=t_theta_2[i]){
    d1<- d1+1
  }
}
print(c(mean(t_theta_1),mean(t_theta_2))) #confidence interval
print(d1/m) #empirical coverage probability

# second case:normal distribution
for(i in 1:m){
  b<-rnorm(n2)
  y_2[i]<-mean(b)
  sd_2[i]<-sd(b)
}
n_theta_1<-y_2-sd_2*z_0.025/sqrt(n2)
n_theta_2<-y_2+sd_2*z_0.025/sqrt(n2)
d2<-0
for(i in 1:m){
  if(n_theta_1[i]<=mu & mu<=n_theta_2[i]){
    d2<- d2+1
  }
}
print(c(mean(n_theta_1),mean(n_theta_2))) #confidence interval
print(d2/m) #empirical coverage probability
```

the ECP(empirical coverage probability) of first case is taller than 0.95;the ECP(empirical coverage probability) of second case is lower than 0.95.because second case is a Approximate distribution,first case is a Precise distribution.so using t distribution with samller sample size is more efficient than using normal distribution with bigger sample size.


## Question 6.5

 Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
χ2(2) data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

## Answer

According to several knowledge of statistics,a 95% symmetric t-interval  applied to estimate a mean from normal distribution is
$$
\left(\bar{Y_i}-\frac{S* t_{\frac{\alpha}{2}}(n-1)}{\sqrt{n}},\bar{Y_i}+\frac{S* t_{\frac{\alpha}{2}}(n-1)}{\sqrt{n}}\right)
$$
noting:
$$
\bar{Y_i}=\frac{1}{n}\sum_{i=1}^n Y_i;S=\sqrt{S^2}=\sqrt{\frac{\sum_{i=1}^n(Y_i-\bar{Y})}{n-1}}
$$
then we keep the format of this confidence interval,making $Y_i\overset{iid}{\sim}\chi^2(2);E(Y_i)=2$

codes as follows:

```{r}
m<- 1e4;n<-20;sd<-numeric(m);y<-numeric(m);t_0.025_19<- 2.093
for(i in 1:m){
  a<- rchisq(n,2)
  y[i]<- mean(a)
  sd[i]<- sd(a)
}
theta_1<- y-sd*t_0.025_19/sqrt(n)
theta_2<- y+sd*t_0.025_19/sqrt(n)
d<-0
for(i in 1:m){
  if(theta_1[i]<=2 & 2<=theta_2[i]){
    d<- d+1
  }
}# the mean of chis-quared is 2
print(d/m)
```
compared with simulation results in Example 6.4.The t-interval should be more robust to departures from normality than the interval for variance.



## Question 6.7

Estimate the power of the skewness test of normality against symmetric
Beta($\alpha$, $\alpha$) distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as t(ν)?

## Answer

we put the results of skewness test of normality against symmetric Beta($\alpha$, $\alpha$) distributions and the results of skewness test of normality against t(ν) together.

before simulation,we guess that both results are poor,because  If the distribution of X is normal, then $\sqrt{b_1}$ is asymptotically normal with mean 0 and variance 6/n(or 6(n-2)/(n+1)*(n+3)).
$$
\sqrt{b_1}=\frac{\frac{1}{n}\sum_{i=1}^n (X_i -\bar{X})^3}{(\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2)^{\frac{3}{2}}}
$$
but both distributions in this exercise are not normal distribution.

codes as follows:
```{r}
alpha<-0.05 
m<-1e4 
a<-seq(1,241,4) 
n<-length(a)
cv <- qnorm(0.975, 0, sqrt(6*(n-2) / ((n+1)*(n+3)))) 
eop<-numeric(length(a)) 
eop1<-numeric(length(a))
sk <- function(x) {
  #computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}
for(j in 1:n){
  sr<-numeric(m)
  sr1<-numeric(m)
  for(i in 1:m){
    x<-rbeta(n,a[j],a[j])
    x1<-rt(n,a[j])
    sr[i] <- as.integer(abs(sk(x)) > cv)
    sr1[i]<-as.integer(abs(sk(x1))>cv)
  }
  eop[j]<-mean(sr)
  eop1[j]<-mean(sr1)
}
eop
eop1
plot(a, eop, type = "b",ylim=c(0,1))
points(a,eop1,type="b")
lines(eop,col="DarkTurquoise",lty=1)
lines(eop1,col="DeepPink",lty=2)
print(d<-data.frame(eop,eop1))
```

according to the results above,we learn two facts as follows

first:the results accord with our prediction before simulation;in other words,they are poor.

second:when parameters are samll,compared with results of t(v),the results of symmetric Beta($\alpha$, $\alpha$) distributions are significantly different.when parameters are large enough,they are very similar. 


## Question 6.8

Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat{\alpha}$ = 0.055. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

## Answer

As for small sample size,we sampled 20 samples;As for medium sample size,we sampled 35 samples;As for large sample size,we sampled 60 samples.

codes as follows:
```{r}
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}
m<-1e4
sigma1 <- 1
sigma2 <- 1.5
n<-c(20,35,60) 
power<-numeric(length(n));power_f<-numeric(length(n))
for(i in 1:length(n)){
  power[i] <- mean(replicate(m, expr={
    x <- rnorm(n[i], 0, sigma1)
    y <- rnorm(n[i], 0, sigma2)
    count5test(x, y)
  }))# power of  count five test
  pvalues <- replicate(m, expr = {
    x <- rnorm(n[i], 0, sigma1)
    y <- rnorm(n[i], 0, sigma2)
    ftest <- var.test(x,y)
    ftest$p.value } )
  power_f[i]<-mean(pvalues<=0.055)# power of f test
}
k<-data.frame(power,power_f,row.names=c("small sample 20","medium sample 35","big sample 60"))
print(k)
```

According to the results above, no matter how big the sample size is,the power of F test is higher than the power of count five test.

## Question 6.C

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If X and Y are iid, the multivariate
population skewness $\beta_{1,d}$ is defined by Mardia as
$$
\beta_{1,d}=E\left[(X-\mu)^T \Sigma^{-1}(Y-\mu)\right]^3
$$
Under normality,$\beta_{1,d}=0$. The multivariate skewness statistic is
$$
b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^n \left[(X_i-\bar{X})^T \hat{\Sigma}^{-1}(X_j-\bar{X})\right]^3
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $\hat{b_{1,d}}$are significant. The asymptotic distribution of nb1,d/6 is chisquared with d(d + 1)(d + 2)/6 degrees of freedom.

## Answer

first of all,for more convenient solve the problems of example 6.8 and example 6.10,we should create a function to compute the value of multivariate skewness statistics $b_{1,d}$
$$
b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^n \left[(X_i-\bar{X})^T \hat{\Sigma}^{-1}(X_j-\bar{X})\right]^3
$$
codes as follows:
```{r}
cs<-function(data.frame){
  xbar<-numeric()
  for(k in 1:ncol(data.frame)){
    xbar[k]<-mean(data.frame[,k])
  }
  isum<-numeric()
  for(i in 1:nrow(data.frame)){
    jsum<-numeric()
    for(j in 1:nrow(data.frame)){
      jsum[j]<-(as.matrix(data.frame[i,])%*%solve(cov(data.frame))%*%as.matrix(t(data.frame[j,])))^3
    }
    isum[i]<-sum(jsum)
  }
  b<-sum(isum)/nrow(data.frame)^2
  return(b)
}
```

example 6.8 for multivariate

```{r}
#example 6.8 for multivariate
n<-c(30,50)
alpha<-0.05
d<-3
m<-100
cv<-qchisq(1-alpha,d*(d+1)*(d+2)/6)
p_reject<-numeric(length(n))
for(i in 1:length(n)){
  tr<-numeric(m)
  for(j in 1:m){
    x1<-rnorm(n[i])
    x2<-rnorm(n[i])
    x3<-rnorm(n[i])
    s<-data.frame(x1,x2,x3)
    tr[j]<-as.integer(n[i]*cs(s)/6>=cv)
  }
  p_reject[i]<-mean(tr)
}
print(p_reject)
```
According to the results above,there are not enough reasons to reject the null hypothesis(skewness is 0).

example 6.10 for multivariate

```{r}
#example 6.10 for multivriate
epsilon <- c(0.5,0.75)
alpha <- 0.1
n <- 30
m <- 100
d<-3
pw<-numeric(length(epsilon))
cv<-qchisq(1-alpha,d*(d+1)*(d+2)/6)
for(i in 1:length(epsilon)){
  tr<-numeric(m)
  for(j in 1:m){
    sigma <- sample(c(1, 10), replace = TRUE,
                    size = n, prob = c(1-epsilon[i], epsilon[i]))
    x1 <- rnorm(n, 0, sigma)
    x2 <- rnorm(n, 0, sigma)
    x3 <- rnorm(n, 0, sigma)
    s<-data.frame(x1,x2,x3)
    tr[j]<-as.integer(n*cs(s)/6>=cv)
  }
  pw[i]<-mean(tr)
}
print(pw)
```
according to the results above,there is a better result when $\epsilon=0.5$.



## Discussion

1,If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one
method and 0.676 for another method. Can we say the powers
are different at 0.05 level?

Answer1

we cannot say the power are different at 0.05 level.

2,What is the corresponding hypothesis test problem?

Answer2

the problem maybe there is too small sample size.

3,What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test?

Answer3

except two-sample t-test,we can employ the others.because the precondition of using two-sample t-test is there are two independent  samples.

4,What information is needed to test your hypothesis?

Answer4

adequate samples;better characteristic statistics.


## Question 7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer

$$
\hat{bias_{jack}}=(n-1)\left(\bar{\theta_{(.)}}-\hat{\theta}\right)
$$
$$
\hat{se_{jack}}=\sqrt{(n-1)\frac{1}{n}\sum_{i=1}^n (\hat{\theta_{(i)}}-\bar{\theta_{(.)}})}
$$
$\hat{\theta}$ is an estimator from original sample

$\hat{\theta_{(i)}}$ is the ith estimator of the ith jackknife sample

$$\bar{\theta_{(.)}}=\frac{1}{n}\sum_{i=1}^n \hat{\theta_{(i)}}$$
codes as follows:

```{r}
#jackknife estimate of the bias of the correlation statistic in Example 7.2.
LSAT<-c(576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594)
GPA<-c(339, 330, 281, 303, 344, 307, 300, 343, 336, 313, 312, 274, 276, 288, 296)
cor_jack<-numeric(length(LSAT))#generate a container of values
for(i in 1:length(LSAT)){
  cor_jack[i]<-cor(LSAT[-i],GPA[-i])
}#compute the jackknife replicates, leave-one-out estimates
bais_jack<-(length(LSAT)-1)*(mean(cor_jack)-cor(LSAT,GPA))
print(bais_jack)
#jackknife estimate of the standard error of the correlation statistic in Example 7.2.
se <- sqrt((length(LSAT)-1) *mean((cor_jack-mean(cor_jack))^2))
print(se)
```



## Question 7.5

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures $1/\lambda$ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.

## Answer

codes as follows:
```{r}
library(boot)
time<-c(3,5,7,18,43,85,91,98,100,130,230,487)
boot_obj_1 <- boot(time, R = 2000,
                 statistic = function(x, i){return(mean(x[i]))})
print(boot.ci(boot_obj_1, type=c("basic","norm","perc","bca")))
```
According to the results above,it is Obvious that there are different results from different methods.The standard bootstrap confidence interval based on asymptotic normality;The basic bootstrap confidence intervals based on the large sample property;Percentile CI (percent) by assuming $\hat{\theta*}$
and $\hat{\theta}$ have approximately the same distribution;Better bootstrap confidence intervals are a modified version of percentile intervals,with Adjustments to percentile.this is why their results are different.



## Question 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.

## Answer

$$
\hat{bias_{jack}}=(n-1)\left(\bar{\theta_{(.)}}-\hat{\theta}\right)
$$

$$
\hat{se_{jack}}=\sqrt{(n-1)\frac{1}{n}\sum_{i=1}^n (\hat{\theta_{(i)}}-\bar{\theta_{(.)}})}
$$
$\hat{\theta}$ is an estimator from original sample

$\hat{\theta_{(i)}}$ is the ith estimator of the ith jackknife sample

$$\bar{\theta_{(.)}}=\frac{1}{n}\sum_{i=1}^n \hat{\theta_{(i)}}$$

codes as follows:

```{r}
library(bootstrap)
data(scor, package = "bootstrap")
a<-eigen(cov(scor))
ht<-a$values[1]/sum(a$values)
ht_jack<-numeric(nrow(scor))
for(i in 1:nrow(scor)){
  ht_jack[i]<-eigen(cov(scor[-i,]))$values[1]/sum(eigen(cov(scor[-i,]))$values)
}
bias_jack<-(nrow(scor)-1)*(mean(ht_jack)-ht)
se_jack<-sqrt((nrow(scor)-1)*mean((ht_jack-mean(ht_jack))^2))
print(c(bias_jack,se_jack))
```




## Question 7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer

in analogy with leave-one-out (n-fold) cross validation,as for leave-two-out cross validation,we set some settings as follows.

there are $C_n^2$ kinds of methods to choose two samples as test points;hence,we stipulate the test points from original samples are (n,1), (k,k+1),k=1,2....n-1,noting: (k,k+1) denotes setting the kth and the (k+1)th as test points.And the prediction error $e_k=|y_k-\hat{y_k}|+|y_{k+1}-\hat{y_{k+1}}|$ and $e=\frac{1}{n}\sum_{k=1}^n e_k^2$

coeds as follows:
```{r}
library("DAAG")
data(ironslag,package="DAAG")
magnetic<-ironslag$magnetic
chemical<-ironslag$chemical
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# fit models on leave-two-out samples
for (k in 1:n-1) {
  y <- magnetic[-c(k,k+1)]
  x <- chemical[-c(k,k+1)]
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[c(k,k+1)]
  e1[k] <- abs(magnetic[k] - yhat1[1])+abs(magnetic[k+1]-yhat1[2])
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[c(k,k+1)] +
    J2$coef[3] * chemical[c(k,k+1)]^2
  e2[k] <- abs(magnetic[k] - yhat2[1])+abs(magnetic[k+1]-yhat1[2])
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[c(k,k+1)]
  yhat3 <- exp(logyhat3)
  e3[k] <- abs(magnetic[k] - yhat3[1])+abs(magnetic[k+1]-yhat1[2])
  J4 <- lm(log(y) ~ log(x))
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[c(k,k+1)])
  yhat4 <- exp(logyhat4)
  e4[k] <- abs(magnetic[k] - yhat4[1])+abs(magnetic[k+1]-yhat1[2])
}
y_1<-magnetic[-c(1,n)]
x_1<-chemical[-c(1,n)]
J1_1<-lm(y_1~x_1)
yhat1_1<-J1_1$coef[1] + J1_1$coef[2] * chemical[c(1,n)]
e1[n]<-abs(magnetic[1]-yhat1_1[1])+abs(magnetic[n]-yhat1_1[2])
J2_1<-lm(y_1~x_1+I(x_1^2))
yhat2_1<-J2_1$coef[1] + J2_1$coef[2] * chemical[c(1,n)] +
  J2_1$coef[3] * chemical[c(1,n)]^2
e2[n]<-abs(magnetic[1]-yhat2_1[1])+abs(magnetic[n]-yhat2_1[2])
J3_1 <- lm(log(y_1) ~ x_1)
logyhat3_1 <- J3_1$coef[1] + J3_1$coef[2] * chemical[c(1,n)]
yhat3_1 <- exp(logyhat3_1)
e3[n]<-abs(magnetic[1]-yhat3_1[1])+abs(magnetic[n]-yhat3_1[2])
J4_1 <- lm(log(y_1) ~ log(x_1))
logyhat4_1 <- J4_1$coef[1] + J4_1$coef[2] * log(chemical[c(1,n)])
yhat4_1 <- exp(logyhat4_1)
e4[n] <- abs(magnetic[1]-yhat4_1[1])+abs(magnetic[n]-yhat4_1[2])
print(c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2)))
```
According to the results above,the second model is the best.
```{r}
print(L2 <- lm(magnetic ~ chemical + I(chemical^2)))
```

$\therefore$
$$
\hat{Y}=24.49262+(-1.39334)X+0.05452X^2
$$


## Question 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Answer

suppose $X,Y\sim N(0,1)$,the sample size of X is 10,the sample size of Y is 15;null hypothesis is there are equal variances,alternative hypthesis is there are not equal variances.


codes as follows:
```{r}
m<-999;n1<- 10;n2<- 15
set.seed(12345)
x1<-rnorm(n1);x2<-rnorm(n2)
z<-c(x1,x2)
out1 <- sum(x1 > max(x2)) + sum(x1 < min(x2))
out2 <- sum(x2 > max(x1)) + sum(x2 < min(x1))
t0<-max(c(out1, out2))
d<-numeric(m)
for(i in 1:m){
  q<-sample(1:25,size=10,replace=FALSE)
  y1<-z[q]
  y2<-z[-q]
  out_1 <- sum(y1 > max(y2)) + sum(y1 < min(y2))
  out_2 <- sum(y2 > max(y1)) + sum(y2 < min(y1))
  d[i]<- max(c(out_1, out_2))
}
p<-mean(c(t0,d)>5)
print(p)
```
According to the results above,we can not reject the null hypothesis.the result accords with our assumption.


## Question 

Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

1,Unequal variances and equal expectations

2,Unequal variances and unequal expectations

3,Non-normal distributions: t distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions)

4,Unbalanced samples (say, 1 case versus 10 controls)

## Answer

codes as follows:
```{r}
library(boot);library(RANN);library(energy);library(Ball)
set.seed(1);n<-50
p.value_nn<-p.value_energy<-p.value_ball<-numeric(5)# the container of p.value
Tn3 <- function(z, ix, sizes) {
  n1 <- sizes[1]
  n2 <- sizes[2]
  n <- n1 + n2
  o <- rep(0, length(z))
  z <- as.data.frame(cbind(z, o))
  z <- z[ix, ]
  NN <- nn2(z, k=4)
  block1 <- NN$nn.idx[1:n1, ]
  block2 <- NN$nn.idx[(n1+1):n, ]
  i1 <- sum(block1 < n1 + .5)
  i2 <- sum(block2 > n1 + .5)
  return((i1 + i2) / (4 * n))
}
## for question 1
x1<-rnorm(n,0,1);y1<-rnorm(n,0,2)#Unequal variances and equal expectations
x <- as.vector(x1)
y <- as.vector(y1)
z <- c(x, y)
# as for nn
N <- c(50,50)
boot.obj <- boot(data = z, statistic = Tn3,
                 sim = "permutation", R = 999, sizes = N)
tb <- c(boot.obj$t, boot.obj$t0)
p.value_nn[1]<-mean(tb >= boot.obj$t0)
# as for energy
boot.obs <- eqdist.etest(z, sizes=N, R=999)
p.value_energy[1] <- boot.obs$p.value
# as for ball
obq <- bd.test(x = x, y = y, R=999)
p.value_ball[1]<-obq$p.value
## for question 2
x2<-rnorm(n,1,1);y2<-rnorm(n,0,2)#Unequal variances and expectations
x <- as.vector(x2)
y <- as.vector(y2)
z <- c(x, y)
# as for nn
N <- c(50,50)
boot.obj <- boot(data = z, statistic = Tn3,
                 sim = "permutation", R = 999, sizes = N)
tb <- c(boot.obj$t, boot.obj$t0)
p.value_nn[2]<-mean(tb >= boot.obj$t0)
# as for energy
boot.obs <- eqdist.etest(z, sizes=N, R=999)
p.value_energy[2] <- boot.obs$p.value
# as for ball
obq <- bd.test(x = x, y = y, R=999)
p.value_ball[2]<-obq$p.value
## for the first part of question 3
x3_1<-rt(n,1);y3_1<-rt(n,1)#t distribution with 1 df
x <- as.vector(x3_1)
y <- as.vector(y3_1)
z <- c(x, y)
# as for nn
N <- c(50,50)
boot.obj <- boot(data = z, statistic = Tn3,
                 sim = "permutation", R = 999, sizes = N)
tb <- c(boot.obj$t, boot.obj$t0)
p.value_nn[3]<-mean(tb >= boot.obj$t0)
# as for energy
boot.obs <- eqdist.etest(z, sizes=N, R=999)
p.value_energy[3] <- boot.obs$p.value
# as for ball
obq <- bd.test(x = x, y = y, R=999)
p.value_ball[3]<-obq$p.value
## for the second part of question 3
#suppose the mixture of two normal distributions is 0.2*N(1,1)+0.8*N(0,2)
mean<-sample(c(0,1),replace=TRUE,size=n,prob=c(0.8,0.2))
sigma<-sample(c(1,2),replace=TRUE,size=n,prob=c(0.2,0.8))
x3_2<-rnorm(n,mean,sigma);y3_2<-rnorm(n,mean,sigma)#mixture of two normal distributions
x <- as.vector(x3_2)
y <- as.vector(y3_2)
z <- c(x, y)
# as for nn
N <- c(50,50)
boot.obj <- boot(data = z, statistic = Tn3,
                 sim = "permutation", R = 999, sizes = N)
tb <- c(boot.obj$t, boot.obj$t0)
p.value_nn[4]<-mean(tb >= boot.obj$t0)
# as for energy
boot.obs <- eqdist.etest(z, sizes=N, R=999)
p.value_energy[4] <- boot.obs$p.value
# as for ball
obq <- bd.test(x = x, y = y, R=999)
p.value_ball[4]<-obq$p.value
## for question 4
x4<-rnorm(n);y4<-rnorm(n-10)#Unbalanced samples
x <- as.vector(x4)
y <- as.vector(y4)
z <- c(x, y)
# as for nn
N <- c(50,40)
boot.obj <- boot(data = z, statistic = Tn3,
                 sim = "permutation", R = 999, sizes = N)
tb <- c(boot.obj$t, boot.obj$t0)
p.value_nn[5]<-mean(tb >= boot.obj$t0)
# as for energy
boot.obs <- eqdist.etest(z, sizes=N, R=999)
p.value_energy[5] <- boot.obs$p.value
# as for ball
obq <- bd.test(x = x, y = y, R=999)
p.value_ball[5]<-obq$p.value
# to sum up
condition<-c("Unequal variances","Unequal variances and mean","t distribution with 1 df"
     ,"mixture of two normal distributions","Unbalanced samples")
s<-data.frame(condition,p.value_nn,p.value_energy,p.value_ball)
print(s)
```

According to the results above:as for Unequal variances,the p.value of energy is best;as for Unequal variances and mean,the p.value of energy is the best;as for t distribution with 1 df,the p.value of ball is the best;as for mixture of two normal distributions,the p.value of energy is the best;as for unbalanced samples,the p.value of energy is the best.what is more,all p.values accord with our hypotheses;in other words, the results above are right.


## Question 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

## Answer

$standard\ Laplace\ distribution$:
$$
f(x)=\frac{1}{2} e^{|x|},x\in R
$$
set $g\left(.|X_t\right)$ is the density function of $N(X_t,\sigma^2)$

compute $r(X_t,Y)$:
$$
\begin{align}
r(X_t,Y)&=\frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}\\
&=\frac{f(Y)}{f(X_t)}\\
&=e^{|X_t|-|Y|}
\end{align}
$$
codes as follows:
```{r}
rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= exp(abs(x[i-1])-abs(y)))
      x[i] <- y 
    else {
        x[i] <- x[i-1]
        k <- k + 1
      } }
  return(list(x=x, k=k))
}
N <- 2000 #the length of chain
sigma <- c(0.05, 0.5, 2, 16) # parameters of normal distribution
x0<-1
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k/N, rw2$k/N, rw3$k/N, rw4$k/N)) # k/N denotes rejection rate.
par(mfrow=c(2,2))
win.graph(width=4.875, height=2.5,pointsize=8)
plot(rw1$x,type="l",main="",xlab="sigma=0.05",ylab = "x1")
plot(rw2$x,type="l",main="",xlab="sigma=0.5",ylab = "x2")
plot(rw3$x,type="l",main="",xlab="sigma=2",ylab = "x3")
plot(rw4$x,type="l",main="",xlab="sigma=16",ylab = "x4")
```

According to the results above,Only the second and third chain has a rejection rate in the range [0.15, 0.5].And there is the best performance when sigma is equal to 2.

## Question 

For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.

## Answer

The target distribution is standard Laplace distribution,and the proposal distribution is $N(X_t,\sigma^2)$.

The scalar summary statistic $\psi_{ij}$ is the mean of the ith chain up to time j.

codes as follows:
```{r}
# the first step:generate laplace chains
dn<-function(x,y){
  z<-exp(abs(x)-abs(y))
  return(z)
}#
laplace.chain <- function(sigma, N, X1) {
  #Implement a random walk Metropolis sampler for generating the
  #standard Laplace distribution
  #with Normal(X[t], sigma) proposal distribution
  #and starting value X1
  x <- numeric(N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma) #candidate point
    if (u[i] <= dn(x[i-1],y))
      x[i] <- y 
    else {
      x[i] <- x[i-1]
    }
  }
  return(x)
}
# the second step:compute Gelman.Rubin statistic(grs)
# The scalar summary statistic is the
# mean of the ith chain up to time j.
grs <- function(psi) {
  # psi[i,j] is the sum of X[i,1:j]
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}
# the third step:set relevant parameter
sigma <- 2 #parameter of proposal distribution
#according to the exercise above,there is the best performance
#when sigma is equal to 2.
k <- 4 #number of chains to generate
n <- 15000 #length of chains
b <- 1000 #burn-in length
#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- laplace.chain(sigma, n, x0[i])
#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(grs(psi))
#plot psi for the four chains
par(mfrow=c(2,2))
win.graph(width=4.875, height=2.5,pointsize=8)
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",
       xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1),pin=c(3,3)) #restore default
win.graph(width=4.875, height=2.5,pointsize=8)
#plot the sequence of R-hat statistics
rhat <- numeric(n)
for (j in (b+1):n)
  rhat[j] <- grs(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.1, lty=2)
abline(h=1.2,lty=2)
```

## Question 11.4

Find the intersection points A(k) in (0, $\sqrt{k}$) of the curves
$$
S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)
$$
and
$$
S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^2(k)}{k+1-a^2}}\right)
$$
for k = 4 : 25, 100, 500, 1000, where t(k) is a Student t random variable with k degrees of freedom. 

## Answer

$$
\begin{align}
S_{k-1}(a)-S_{k}(a)&=P\left(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)-P\left(t(k)>\sqrt{\frac{a^2(k)}{k+1-a^2}}\right)\\
&=1-F_{t(k-1)}\left(\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)-\left(1-F_{t(k)}\left(\sqrt{\frac{a^2(k)}{k+1-a^2}}\right)\right)\\
&=F_{t(k)}\left(\sqrt{\frac{a^2(k)}{k+1-a^2}}\right)-F_{t(k-1)}\left(\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)
\end{align}
$$
codes as follows:
```{r}
k<-c(4:25,100,500,1000)
s<-numeric(length(k))
library(rootSolve)
# the third exercise
for(i in 1:length(k)){
  f<-function(a){
    y1<-sqrt(a^2*k[i]/(k[i]+1-a^2))
    y2<-sqrt(a^2*(k[i]-1)/(k[i]-a^2))
    a1<-pt(y1,k[i],log.p = FALSE)
    a2<-pt(y2,k[i]-1,log.p=FALSE)
    return(a1-a2)
  }
  s[i]<- uniroot(f,c(1e-3,sqrt(k[i]-1e-3)))$root
}
print(s)
```



## Question

A-B-O blood type problem

Let the three alleles be A, B, and O.

genotype | AA | BB | OO | AO | BO | AB | Sum
-|-|-|-|-|-|-|-
Frequency | $p^2$ | $q^2$ | $r^2$ | 2pr | 2qr | 2pq | 1
Count | nAA | nBB | nOO | nAO | nBO | nAB | n

Observed data: nA· = nAA + nAO = 444 (A-type),nB· = nBB + nBO = 132 (B-type), nOO = 361 (O-type),nAB = 63 (AB-type).

Use EM algorithm to solve MLE of p and q (consider missing
data nAA and nBB).

Record the values of p and q that maximize the conditional
likelihood in each EM steps, calculate the corresponding
log-maximum likelihood values (for observed data), are they
increasing?

## Answer

set $Z1$ denoting the observations of AA;set $Z2$ denoting the observations of BB.set $f$ denoting the likelihood function weeding out the constant.
$$
\begin{align}
f&=(p^2)^{Z1}(q^2)^{Z2}(r^2)^{n_oo}(2pr)^{n_{A.}-Z1}(2qr)^{n_{B.}-Z2}(2pq)^{n_{AB}}\\
&=p^{Z1+n_{A.}+n_{AB}} q^{Z2+n_{B.}+n_{AB}} r^{2n_{oo}+n_{A.}+n_{B.}-Z1-Z2} 2^{n_{A.}+n_{B.}+n_{AB}-Z1-Z2}
\end{align}
$$
$\therefore$
$$
lnf=(Z1+n_{A.}+n_{AB})lnp+(Z2+n_{B.}+n_{AB})lnq+(2n_{oo}+n_{A.}+n_{B.}-Z1-Z2)lnr
$$
noting:$lnf$ weeded out something that is unrelated with p or q.

$\because$
$$
Z1\sim B\left(n_{A.},\frac{p}{p+2r}\right);
Z2\sim B\left(n_{B.},\frac{q}{q+2r}\right)
$$
$\therefore$ when $n_{A.},n_{B.}$ are available and $p=p^{(i)}$; $q=q^{(i)}$;
 $r=r^{(i)}$
$$
EZ1=n_{A.} \frac{p^{(i)}}{p^{(i)}+2r^{(i)}};EZ2=n_{B.} \frac{q^{(i)}}{q^{(i)}+2r^{(i)}}
$$
set
$$
k1=EZ1+n_{A.}+n_{AB};k2=EZ2+n_{B.}+n_{AB};k3=2n_{oo}+n_{A.}+n_{B.}-EZ1-EZ2
$$
$\therefore$
$$
Elnf=k1lnp+k2lnq+k3ln(1-p-q)
$$
$\therefore$
$$
\frac{\partial Elnf}{\partial p}=\frac{k1}{p}-\frac{k3}{1-p-q}\\
\frac{\partial Elnf}{\partial q}=\frac{k2}{q}-\frac{k3}{1-p-q}
$$
solving the equation set


set $n=n_{A.}+n_{B.}+n_{AB}+n_{oo}$
$$
\begin{align}
p^{(i+1)}&=\frac{k1}{k1+k2+k3}\\
&=\frac{1}{2n}(EZ1+n_{A.}+n_{AB})
\end{align}
$$
$$
\begin{align}
q^{(i+1)}&=\frac{k2}{k1+k2+k3}\\
&=\frac{1}{2n}(EZ2+n_{B.}+n_{AB})
\end{align}
$$
codes as follows:
```{r}
na<-444;nb<-132;nab<-63;noo<-361
n<-na+nb+nab+noo
pf<-function(p,q){
  y<-na*p/(p+2*(1-p-q))
  yy<-(y+na+nab)/(2*n)
  return(yy)
}
qf<-function(p,q){
  y<-nb*q/(q+2*(1-p-q))
  yy<-(y+nb+nab)/(2*n)
  return(yy)
}
p1<-0.5;q1<-0.3
p<-numeric();i<-1;q<-numeric()
while(p1 != pf(p1,q1) | q1 != qf(p1,q1)){
  p[i]<-pf(p1,q1)
  q[i]<-qf(p1,q1)
  i<-i+1
  p1<-pf(p1,q1)
  q1<-qf(p1,q1)
}
print(p1)
print(q1)
print(p)
print(q)
# compute the corresponding
#log-maximum likelihood values
ez1<-na*p/(p+2*(1-p-q))
ez2<-nb*q/(q+2*(1-q-p))
k1<-nab+na+ez1
k2<-nab+nb+ez2
k3<-2*noo+na+nb-ez1-ez2
elnf<-k1*log(p)+k2*log(q)+k3*log(1-p-q)
print(elnf)
plot(elnf,type="l")
```

According to the results above, they are increasing.


## Question

Use both for loops and lapply() to fit linear models to the
mtcars using the formulas stored in this list:
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
```
## Answer

```{r}
attach(mtcars)
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
formulas<- as.character(formulas)
for (i in 1: length(formulas)){
  t<-lm(formulas[i])
  print(t)
}
lapply(formulas,function(x) lm(x))
detach(mtcars)
```



## Question

The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.
```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
```
Extra challenge: get rid of the anonymous function by using
directly
Note: the anonymous function is defined in Section 10.2 (page 181,
Advanced R)

## Answer

```{r}
sapply(trials,function(x) x$p.value)
# extra challenge
x<-numeric()
for(i in 1:length(trials)){
  x[i]<-trials[[i]]$p.value
}
print(x)
```

## Question

Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer

```{r}
attach(mtcars)
f <- function(X, FUN, FUN.VALUE, simplify = FALSE){
  out <- Map(function(x) vapply(x, FUN, FUN.VALUE), X)
  if(simplify == TRUE){return(simplify2array(out))}
  out
}
f(list(mtcars),sum, numeric(1))
detach(mtcars)
```

